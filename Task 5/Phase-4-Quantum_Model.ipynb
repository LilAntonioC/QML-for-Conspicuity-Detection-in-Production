{"cells":[{"cell_type":"markdown","metadata":{},"source":["##"]},{"cell_type":"markdown","metadata":{},"source":["## **Training a deep learning model (Clasical solution)**\n","\n","```javascript\n","Since we made our dataset private on Hugging Face during phase 2, we need to log in to access it.\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:42:27.153386Z","iopub.status.busy":"2024-08-08T05:42:27.153094Z","iopub.status.idle":"2024-08-08T05:42:28.881082Z","shell.execute_reply":"2024-08-08T05:42:28.880152Z","shell.execute_reply.started":"2024-08-08T05:42:27.153359Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["!huggingface-cli login --token \"\""]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-08-08T05:42:08.710636Z","iopub.status.busy":"2024-08-08T05:42:08.710355Z","iopub.status.idle":"2024-08-08T05:42:27.150834Z","shell.execute_reply":"2024-08-08T05:42:27.149803Z","shell.execute_reply.started":"2024-08-08T05:42:08.710610Z"},"id":"CDHGJ75bTQFK","jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting pennylane\n","  Using cached PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: evaluate in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (0.4.2)\n","Requirement already satisfied: numpy<2.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.26.4)\n","Requirement already satisfied: scipy in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (1.13.1)\n","Requirement already satisfied: networkx in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (2.8.8)\n","Collecting rustworkx (from pennylane)\n","  Using cached rustworkx-0.15.1-cp38-abi3-macosx_11_0_arm64.whl.metadata (9.9 kB)\n","Collecting autograd (from pennylane)\n","  Using cached autograd-1.6.2-py3-none-any.whl.metadata (706 bytes)\n","Requirement already satisfied: toml in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (0.10.2)\n","Collecting appdirs (from pennylane)\n","  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n","Collecting semantic-version>=2.7 (from pennylane)\n","  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting autoray>=0.6.11 (from pennylane)\n","  Using cached autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: cachetools in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (5.3.3)\n","Collecting pennylane-lightning>=0.37 (from pennylane)\n","  Downloading PennyLane_Lightning-0.37.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (23 kB)\n","Requirement already satisfied: requests in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (2.32.3)\n","Requirement already satisfied: typing-extensions in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (4.12.2)\n","Requirement already satisfied: packaging in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pennylane) (22.0)\n","Requirement already satisfied: datasets>=2.0.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.20.0)\n","Requirement already satisfied: dill in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: multiprocess in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from evaluate) (0.23.4)\n","Requirement already satisfied: filelock in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from requests->pennylane) (2024.6.2)\n","Requirement already satisfied: future>=0.15.2 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from autograd->pennylane) (0.18.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /Users/liljaco/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Using cached PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n","Using cached autoray-0.6.12-py3-none-any.whl (50 kB)\n","Downloading PennyLane_Lightning-0.37.0-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hUsing cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Using cached autograd-1.6.2-py3-none-any.whl (49 kB)\n","Using cached rustworkx-0.15.1-cp38-abi3-macosx_11_0_arm64.whl (1.7 MB)\n","Installing collected packages: appdirs, semantic-version, rustworkx, autoray, autograd, pennylane-lightning, pennylane\n","Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.12 pennylane-0.37.0 pennylane-lightning-0.37.0 rustworkx-0.15.1 semantic-version-2.10.0\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install pennylane evaluate "]},{"cell_type":"markdown","metadata":{},"source":["## **Imports**"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n","\n","from tqdm import tqdm\n","import pennylane as qml\n","import evaluate\n","from datasets import load_dataset, load_metric\n","from transformers import  (AutoModel, AutoConfig, \n","                          AutoImageProcessor, \n","                          Trainer, TrainingArguments)\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Loading the dataset**\n","\n","```javascript\n","We used the same datset as in the classical version\n","``` "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:50.092372Z","iopub.status.busy":"2024-08-08T05:43:50.091855Z","iopub.status.idle":"2024-08-08T05:43:50.104873Z","shell.execute_reply":"2024-08-08T05:43:50.102557Z","shell.execute_reply.started":"2024-08-08T05:43:50.092322Z"},"id":"DYXNkDSw3K1j","outputId":"214b1966-98cd-4f3b-8494-ee07cff009db","trusted":true},"outputs":[{"data":{"text/plain":["{'image': Image(mode=None, decode=True, id=None),\n"," 'label': ClassLabel(names=['GOOD', 'DAMAGE'], id=None)}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset(\"LaLegumbreArtificial/womanium-balance\")\n","dataset[\"train\"].features"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:50.107430Z","iopub.status.busy":"2024-08-08T05:43:50.106530Z","iopub.status.idle":"2024-08-08T05:43:51.035890Z","shell.execute_reply":"2024-08-08T05:43:51.034960Z","shell.execute_reply.started":"2024-08-08T05:43:50.107395Z"},"id":"0ocTTLfS3K1j","outputId":"513309ab-f980-405c-8f48-390cfac66bcc","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['image', 'label'],\n","        num_rows: 14000\n","    })\n","    test: Dataset({\n","        features: ['image', 'label'],\n","        num_rows: 6000\n","    })\n","})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"markdown","metadata":{},"source":["## **Preprocessing of the data**\n","\n","\n","```java\n","In the next cells same as the classical model we need to preprocess the data to be able to feed it to the hybrid model\n","```"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:51.037876Z","iopub.status.busy":"2024-08-08T05:43:51.037379Z","iopub.status.idle":"2024-08-08T05:43:51.701670Z","shell.execute_reply":"2024-08-08T05:43:51.700500Z","shell.execute_reply.started":"2024-08-08T05:43:51.037837Z"},"id":"VXhbON5O3K1j","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78810fb4b7ef4b4fb79dcaba8feff73b","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31f582308b23442fa73fb9391beaca04","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"]}],"source":["checkpoint = \"google/vit-base-patch16-224\"\n","\n","image_processor  = AutoImageProcessor.from_pretrained(checkpoint)\n","\n","\n","normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n","size = (\n","    image_processor.size[\"shortest_edge\"]\n","    if \"shortest_edge\" in image_processor.size\n","    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",")\n","_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])\n","\n","def transforms(examples):\n","    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n","    del examples[\"image\"]\n","    return examples"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:51.703820Z","iopub.status.busy":"2024-08-08T05:43:51.703192Z","iopub.status.idle":"2024-08-08T05:43:51.720177Z","shell.execute_reply":"2024-08-08T05:43:51.719279Z","shell.execute_reply.started":"2024-08-08T05:43:51.703785Z"},"id":"w-mNLqe73K1j","trusted":true},"outputs":[],"source":["dataset = dataset.with_transform(transforms)"]},{"cell_type":"markdown","metadata":{},"source":["```java\n","In the final result, each image is normalized, cropped, and converted into a tensor, making it ready for input into the model.\n","```"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:51.723243Z","iopub.status.busy":"2024-08-08T05:43:51.721452Z","iopub.status.idle":"2024-08-08T05:43:53.248360Z","shell.execute_reply":"2024-08-08T05:43:53.247168Z","shell.execute_reply.started":"2024-08-08T05:43:51.723207Z"},"id":"vCl3xEgM3K1j","outputId":"5039e136-652c-40f7-c85b-9b727c7f5dda","trusted":true},"outputs":[{"data":{"text/plain":["tensor([[[-0.5922, -0.5843, -0.5765,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5843, -0.5686,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5765, -0.5608,  ..., -0.5294, -0.5373, -0.5373],\n","         ...,\n","         [-0.5765, -0.5608, -0.5373,  ..., -0.3020, -0.3020, -0.2941],\n","         [-0.5765, -0.5529, -0.5373,  ..., -0.3098, -0.3098, -0.2941],\n","         [-0.5686, -0.5451, -0.5373,  ..., -0.3176, -0.3176, -0.3020]],\n","\n","        [[-0.5922, -0.5843, -0.5765,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5843, -0.5686,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5765, -0.5608,  ..., -0.5294, -0.5373, -0.5373],\n","         ...,\n","         [-0.5765, -0.5608, -0.5373,  ..., -0.3020, -0.3020, -0.2941],\n","         [-0.5765, -0.5529, -0.5373,  ..., -0.3098, -0.3098, -0.2941],\n","         [-0.5686, -0.5451, -0.5373,  ..., -0.3176, -0.3176, -0.3020]],\n","\n","        [[-0.5922, -0.5843, -0.5765,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5843, -0.5686,  ..., -0.5373, -0.5451, -0.5451],\n","         [-0.5922, -0.5765, -0.5608,  ..., -0.5294, -0.5373, -0.5373],\n","         ...,\n","         [-0.5765, -0.5608, -0.5373,  ..., -0.3020, -0.3020, -0.2941],\n","         [-0.5765, -0.5529, -0.5373,  ..., -0.3098, -0.3098, -0.2941],\n","         [-0.5686, -0.5451, -0.5373,  ..., -0.3176, -0.3176, -0.3020]]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"train\"][0][\"pixel_values\"]"]},{"cell_type":"markdown","metadata":{},"source":["## **Evaluation metrics**\n","\n","```java\n","In this case, we chose accuracy as the primary metric. Given that the dataset is balanced between the two classes, additional metrics are not necessary at this time.\n","```"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"referenced_widgets":["5070791b3bb640b3874b3bc7c2f67759"]},"execution":{"iopub.execute_input":"2024-08-08T05:43:56.953476Z","iopub.status.busy":"2024-08-08T05:43:56.953183Z","iopub.status.idle":"2024-08-08T05:43:57.684353Z","shell.execute_reply":"2024-08-08T05:43:57.683273Z","shell.execute_reply.started":"2024-08-08T05:43:56.953450Z"},"id":"k76LzYon3K1k","outputId":"5fefc39c-74a9-499e-e666-627555c34c74","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3494300e52504e9b8df3da936fdf6fa7","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["accuracy = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"cell_type":"markdown","metadata":{},"source":["```java\n","A data collator is a tool that helps prepare batches of data for training or testing a model. \n","```"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:53.250558Z","iopub.status.busy":"2024-08-08T05:43:53.249695Z","iopub.status.idle":"2024-08-08T05:43:53.255683Z","shell.execute_reply":"2024-08-08T05:43:53.254176Z","shell.execute_reply.started":"2024-08-08T05:43:53.250528Z"},"id":"oQ_5E2EZ3K1j","trusted":true},"outputs":[],"source":["from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator()"]},{"cell_type":"markdown","metadata":{"id":"eMZB33l-3WmJ"},"source":["## **- Quantum-Enhanced Vision Transformer -**\n","\n","```java\n","In the next two cells, we created a hybrid model that combines a Vision Transformer architecture with a quantum circuit. The Vision Transformer processes the image input, and its output is passed through a dense layer. The resulting features are then fed into a quantum layer, implemented as a quantum circuit with 2 qubits. This quantum layer acts as a final transformation before producing the output logits, which can be used for classification. The quantum circuit is integrated into the model as a custom layer, enabling quantum computations on the features extracted by the Vision Transformer.\n","```\n","\n","# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","\n","```java\n","Quantum Layer description:\n","\n","qml.AngleEmbedding(inputs, wires=range(n_qubits)) - This function embeds classical input data into the quantum circuit by encoding it into the rotation angles of the qubits. It allows the input data to be represented in the quantum state, making it possible for the quantum circuit to process the data.\n","\n","qml.BasicEntanglerLayers(weights, wires=range(n_qubits)) - This template applies a series of entangling operations across the qubits, along with rotations (which can be around the X, Y, or Z axis). Entanglement is a key feature of quantum mechanics that allows qubits to become interconnected and influence each other’s states. The rotations are parameterized by weights, which are trainable parameters with the shape (n_layers, n_qubits). This template makes it easier to create a trainable quantum circuit that can capture complex relationships in the data.\n","\n","return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)] - After processing the input through the quantum circuit, the final quantum states of the qubits are measured. Specifically, the circuit measures the expectation value of the Pauli-Z operator for each qubit. This measurement provides a classical output from the quantum circuit, which can be further processed in the overall model.\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_qubits = 2\n","dev = qml.device(\"default.qubit\", wires=n_qubits)\n","\n","@qml.qnode(dev)\n","def qnode(inputs, weights):\n","    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n","    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n","    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n","\n","n_layers = 6\n","weight_shapes = {\"weights\": (n_layers, n_qubits)}"]},{"cell_type":"markdown","metadata":{},"source":["\n","# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","\n","```java\n","\n","Full Architecture Explanation: Quantum-Enhanced Vision Transformer Model\n","\n","After creating the quantum layer, we developed a hybrid model called `QuantumEnhancedVisionTransformer`. This model is built on the \"google/vit-base-patch16-224\" Vision Transformer as the base architecture. The process flow is as follows:\n","\n","1. Vision Transformer Processing The model first processes the input images using the Vision Transformer. This step extracts high-level features from the images.\n","\n","2. Dropout Layer After obtaining the output from the Vision Transformer, we apply a dropout layer. This helps prevent overfitting by randomly setting a fraction of the input units to zero during training, which promotes generalization.\n","\n","3. Dense Layer The features from the dropout layer are then passed through a dense (fully connected) layer. This layer reduces the dimensionality of the features and prepares them for the next step, ensuring that the most important information is retained.\n","\n","4. Quantum Layer Finally, the processed features are fed into the quantum layer, which applies quantum computations to the data. This quantum layer can potentially capture complex patterns and relationships in the data that classical layers might miss. The output of the quantum layer is used to make the final predictions.\n","\n","This hybrid model leverages the strengths of both classical deep learning (through the Vision Transformer) and quantum computing (through the quantum layer) to perform image classification.\n","```\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:53.257458Z","iopub.status.busy":"2024-08-08T05:43:53.257128Z","iopub.status.idle":"2024-08-08T05:43:56.951934Z","shell.execute_reply":"2024-08-08T05:43:56.950963Z","shell.execute_reply.started":"2024-08-08T05:43:53.257432Z"},"id":"4xkmbw7V3K1j","outputId":"6c7b8140-9dad-4c9a-d8c3-365eb5721486","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f2fbd2fc94841449d48df35d28b5ed6","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["class QuantumEnhancedVisionTransformer(nn.Module):\n","    def __init__(self, checkpoint, num_labels):\n","        super(QuantumEnhancedVisionTransformer, self).__init__()\n","        self.num_labels = num_labels\n","\n","        # Create the model layers\n","        self.config = AutoConfig.from_pretrained(checkpoint, output_attentions=True, output_hidden_states=True)\n","        self.model = AutoModel.from_pretrained(checkpoint, config=self.config)\n","        self.dropout = nn.Dropout(0.1)\n","        self.dense1 = nn.Linear(self.config.hidden_size, num_labels)\n","        self.qlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\n","\n","\n","    def forward(self, pixel_values=None, labels=None):\n","        if pixel_values is None:\n","            raise ValueError(\"Wrong input\")\n","\n","        # Create the flow \n","        outputs = self.model(pixel_values=pixel_values)\n","        pooled_output = outputs.pooler_output  \n","\n","        # add custom layers\n","        dropout_output = self.dropout(pooled_output)\n","        dense_output = self.dense1(dropout_output)\n","        logits = self.qlayer(dense_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if loss is not None:\n","            return loss, logits\n","        else:\n","            return logits\n","\n","# parameters of the model\n","num_labels = 2\n","num_epochs = 5\n","\n","# Create an object of the model\n","model = QuantumEnhancedVisionTransformer(checkpoint=checkpoint, num_labels=num_labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Training and parameters**\n","\n","```java\n","Here we defined the hyperparameters used for the model, such as the learning rate, optimizer settings, number of epochs, and batch sizes for both training and evaluation:\n","\n","For simplification and also comparison we used the same parameters as the classical model\n","```"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T05:43:57.688036Z","iopub.status.busy":"2024-08-08T05:43:57.687735Z","iopub.status.idle":"2024-08-08T06:49:49.884592Z","shell.execute_reply":"2024-08-08T06:49:49.883342Z","shell.execute_reply.started":"2024-08-08T05:43:57.688011Z"},"id":"-K4_YAEn3K1k","outputId":"c074819a-70af-4f77-f4c7-e507fc086e6c","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.6 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240808_060023-d427se9t</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jose-contreras-itj/huggingface/runs/d427se9t' target=\"_blank\">Model_custom_pythorch_Q1</a></strong> to <a href='https://wandb.ai/jose-contreras-itj/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jose-contreras-itj/huggingface' target=\"_blank\">https://wandb.ai/jose-contreras-itj/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jose-contreras-itj/huggingface/runs/d427se9t' target=\"_blank\">https://wandb.ai/jose-contreras-itj/huggingface/runs/d427se9t</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='545' max='545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [545/545 49:00, Epoch 4/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.440400</td>\n","      <td>0.394224</td>\n","      <td>0.956167</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.432300</td>\n","      <td>0.372869</td>\n","      <td>0.967833</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.399300</td>\n","      <td>0.357715</td>\n","      <td>0.981333</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=545, training_loss=0.45231932434467, metrics={'train_runtime': 3945.5042, 'train_samples_per_second': 17.742, 'train_steps_per_second': 0.138, 'total_flos': 0.0, 'train_loss': 0.45231932434467, 'epoch': 4.9771689497716896})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["training_args = TrainingArguments(\n","    output_dir=f\"Model_custom_pythorch_Q1\",\n","    remove_unused_columns=False,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=4,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=5,\n","    warmup_ratio=0.1,\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    do_train=True,\n","    do_eval=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"test\"],\n","    tokenizer=image_processor,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T08:33:58.962337Z","iopub.status.busy":"2024-08-08T08:33:58.961946Z","iopub.status.idle":"2024-08-08T08:34:13.162657Z","shell.execute_reply":"2024-08-08T08:34:13.161466Z","shell.execute_reply.started":"2024-08-08T08:33:58.962299Z"},"id":"NwLiBYwv3K1n","outputId":"d91c1cbe-f257-4b93-89f4-8a38000c729f","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee183536162d47fb8d0c70fb4b1a6a3c","version_major":2,"version_minor":0},"text/plain":["model_weights.pth:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/LaLegumbreArtificial/Model_custom_pythorch_Q1/commit/341e693c1aa779060d6fdf07d3ed43a97df0a07a', commit_message='End of training', commit_description='', oid='341e693c1aa779060d6fdf07d3ed43a97df0a07a', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","metadata":{},"source":["## **Save the weigths**\n","\n","```java\n","This section was created to show that we saved the model\n","```"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T08:33:47.832392Z","iopub.status.busy":"2024-08-08T08:33:47.831670Z","iopub.status.idle":"2024-08-08T08:33:48.375145Z","shell.execute_reply":"2024-08-08T08:33:48.373867Z","shell.execute_reply.started":"2024-08-08T08:33:47.832359Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), \"/kaggle/working/Model_custom_pythorch_Q1/model_weights.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["## **Load the model**"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T07:02:19.608757Z","iopub.status.busy":"2024-08-08T07:02:19.608304Z","iopub.status.idle":"2024-08-08T07:02:20.390548Z","shell.execute_reply":"2024-08-08T07:02:20.387968Z","shell.execute_reply.started":"2024-08-08T07:02:19.608724Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["CustomVisionModel(\n","  (model): ViTModel(\n","    (embeddings): ViTEmbeddings(\n","      (patch_embeddings): ViTPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","      )\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViTEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViTLayer(\n","          (attention): ViTSdpaAttention(\n","            (attention): ViTSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViTSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViTIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViTOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): ViTPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (dense1): Linear(in_features=768, out_features=2, bias=True)\n","  (qlayer): <Quantum Torch Layer: func=qnode>\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize the model architecture\n","model_2 = QuantumEnhancedVisionTransformer(checkpoint=checkpoint, num_labels=num_labels)\n","\n","# Load the saved weights\n","model_2.load_state_dict(torch.load(\"model_weights.pth\"))\n","model_2.eval()  # Set the model to evaluation mode"]},{"cell_type":"markdown","metadata":{},"source":["## **Predictions for testing**\n","\n","```java\n","This two last cells were just to show how to do predictions with the model in this case we need to preprocess the data with the transforms() function and then give it to the model\n","```"]},{"cell_type":"code","execution_count":31,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-08-08T07:22:14.851791Z","iopub.status.busy":"2024-08-08T07:22:14.851401Z","iopub.status.idle":"2024-08-08T08:22:16.747004Z","shell.execute_reply":"2024-08-08T08:22:16.745704Z","shell.execute_reply.started":"2024-08-08T07:22:14.851758Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DONE 0\n","DONE 100\n","DONE 200\n","DONE 300\n","DONE 400\n","DONE 500\n","DONE 600\n","DONE 700\n","DONE 800\n","DONE 900\n","DONE 1000\n","DONE 1100\n","DONE 1200\n","DONE 1300\n","DONE 1400\n","DONE 1500\n","DONE 1600\n","DONE 1700\n","DONE 1800\n","DONE 1900\n","DONE 2000\n","DONE 2100\n","DONE 2200\n","DONE 2300\n","DONE 2400\n","DONE 2500\n","DONE 2600\n","DONE 2700\n","DONE 2800\n","DONE 2900\n","DONE 3000\n","DONE 3100\n","DONE 3200\n","DONE 3300\n","DONE 3400\n","DONE 3500\n","DONE 3600\n","DONE 3700\n","DONE 3800\n","DONE 3900\n","DONE 4000\n","DONE 4100\n","DONE 4200\n","DONE 4300\n","DONE 4400\n","DONE 4500\n","DONE 4600\n","DONE 4700\n","DONE 4800\n","DONE 4900\n","DONE 5000\n","DONE 5100\n","DONE 5200\n","DONE 5300\n","DONE 5400\n","DONE 5500\n","DONE 5600\n","DONE 5700\n","DONE 5800\n","DONE 5900\n","DONE 6000\n","DONE 6100\n","DONE 6200\n","DONE 6300\n","DONE 6400\n","DONE 6500\n","DONE 6600\n","DONE 6700\n","DONE 6800\n","DONE 6900\n","DONE 7000\n","DONE 7100\n","DONE 7200\n","DONE 7300\n","DONE 7400\n","DONE 7500\n","DONE 7600\n","DONE 7700\n","DONE 7800\n","DONE 7900\n","DONE 8000\n","DONE 8100\n","DONE 8200\n","DONE 8300\n","DONE 8400\n","DONE 8500\n","DONE 8600\n","DONE 8700\n","DONE 8800\n","DONE 8900\n","DONE 9000\n","DONE 9100\n","DONE 9200\n","DONE 9300\n","DONE 9400\n","DONE 9500\n","DONE 9600\n","DONE 9700\n","DONE 9800\n","DONE 9900\n","DONE 10000\n","DONE 10100\n","DONE 10200\n","DONE 10300\n","DONE 10400\n","DONE 10500\n","DONE 10600\n","DONE 10700\n","DONE 10800\n","DONE 10900\n","DONE 11000\n","DONE 11100\n","DONE 11200\n","DONE 11300\n","DONE 11400\n","DONE 11500\n","DONE 11600\n","DONE 11700\n","DONE 11800\n","DONE 11900\n","DONE 12000\n","DONE 12100\n","DONE 12200\n","DONE 12300\n","DONE 12400\n","DONE 12500\n","DONE 12600\n","DONE 12700\n","DONE 12800\n","DONE 12900\n","DONE 13000\n","DONE 13100\n","DONE 13200\n","DONE 13300\n","DONE 13400\n","DONE 13500\n","DONE 13600\n","DONE 13700\n","DONE 13800\n","DONE 13900\n"]}],"source":["prediction_arr = []\n","for i in range(len(dataset[\"train\"])):\n","    if i % 100 == 0:\n","        print(f\"DONE {i}\")\n","    pixel_values = dataset[\"train\"][i][\"pixel_values\"]\n","    with torch.no_grad():\n","        outputs = model_2(pixel_values=pixel_values.reshape(1,3,224,224))\n","        logits = outputs if isinstance(outputs, torch.Tensor) else outputs[1]\n","        predictions = torch.argmax(logits, dim=-1)\n","\n","\n","    prediction_arr.append(predictions.item())\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Final accuracy**\n","\n","```java\n","This is not the last test of the model we need a final phase to determine how good was the model\n","```"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-08-08T08:25:38.652799Z","iopub.status.busy":"2024-08-08T08:25:38.651932Z","iopub.status.idle":"2024-08-08T08:25:38.685721Z","shell.execute_reply":"2024-08-08T08:25:38.684944Z","shell.execute_reply.started":"2024-08-08T08:25:38.652766Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.9883571428571428"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import accuracy_score\n","\n","accuracy_score(dataset[\"train\"][\"label\"], prediction_arr)"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
